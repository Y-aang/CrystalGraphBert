{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.CrystalGraph import CrystalGraph\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.analysis.local_env import LocalStructOrderParams ,VoronoiNN\n",
    "import pymatgen.core as mg\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pymatgen\n",
    "from pymatgen.analysis.local_env import BrunnerNN_real\n",
    "from pymatgen.analysis.local_env import NearNeighbors\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABX_position(structure : pymatgen.core.structure.Structure) -> dict:\n",
    "    '''\n",
    "        Input:\n",
    "            structure\n",
    "        Output:\n",
    "            dict{\n",
    "                'A':[pymatgen.core.periodic_table.Species], \n",
    "                'B':[pymatgen.core.periodic_table.Species], \n",
    "                'X':[pymatgen.core.periodic_table.Species]\n",
    "                }\n",
    "        discription:\n",
    "            Counting the number of each species to determine X position, which has the\n",
    "            greatest number.\n",
    "            Using the data of ionic radius (ionic_radius) from pymatgen to determine \n",
    "            A & B of the species in the structure. If ionic radius isn't available, \n",
    "            we use the average cationic radius (average_cationic_radius) instead.\n",
    "    '''\n",
    "    Apos = []\n",
    "    Bpos = []\n",
    "    Xpos = []\n",
    "    has_ionic_radius = True\n",
    "    All_species = list(set(structure.species))\n",
    "    All_species = sorted(All_species, key = lambda t: structure.species.count(t))\n",
    "    Xpos.append(All_species[-1])\n",
    "    All_species.remove(Xpos[0])\n",
    "    \n",
    "    # for item in All_species:\n",
    "    #     if item.ionic_radius is None:\n",
    "    #         has_ionic_radius = False\n",
    "    has_ionic_radius = False\n",
    "    \n",
    "    if has_ionic_radius:\n",
    "        All_species = sorted(All_species, key=lambda t: t.ionic_radius)\n",
    "    else:\n",
    "        All_species = sorted(All_species, key=lambda t: t.average_cationic_radius)\n",
    "        \n",
    "    if len(All_species) == 2:\n",
    "        Apos.append(All_species[1])\n",
    "        Bpos.append(All_species[0])\n",
    "    elif len(All_species) == 3:\n",
    "        if has_ionic_radius:\n",
    "            mid = (All_species[0].ionic_radius + All_species[2].ionic_radius) / 2\n",
    "            if All_species[1].ionic_radius < mid:\n",
    "                Apos.append(All_species[2])\n",
    "                Bpos.append(All_species[1])\n",
    "                Bpos.append(All_species[0])\n",
    "            else:\n",
    "                Apos.append(All_species[2])\n",
    "                Apos.append(All_species[1])\n",
    "                Bpos.append(All_species[0])\n",
    "        else:\n",
    "            mid = (All_species[0].average_cationic_radius + All_species[2].average_cationic_radius) / 2\n",
    "            if All_species[1].average_cationic_radius < mid:\n",
    "                Apos.append(All_species[2])\n",
    "                Bpos.append(All_species[1])\n",
    "                Bpos.append(All_species[0])\n",
    "            else:\n",
    "                Apos.append(All_species[2])\n",
    "                Apos.append(All_species[1])\n",
    "                Bpos.append(All_species[0])\n",
    "    else:\n",
    "        Apos.append(All_species[3])\n",
    "        Apos.append(All_species[2])\n",
    "        Bpos.append(All_species[1])\n",
    "        Bpos.append(All_species[0])\n",
    "    \n",
    "    return {'A' : Apos, 'B' : Bpos, 'X' : Xpos}\n",
    "\n",
    "def coordination_number(structure : pymatgen.core.structure.Structure\n",
    "                        , ABX_dict : dict) -> list:\n",
    "    '''\n",
    "        Input:\n",
    "            structure,\n",
    "            ABX_dict    --  the result of ABX_position()\n",
    "        Output:\n",
    "            list[int]   -- a list of coordination number for each site in structure\n",
    "    '''\n",
    "    ret_list = []\n",
    "    vnn = VoronoiNN()\n",
    "    for i, site in enumerate(structure.sites):\n",
    "        if site.specie in ABX_dict['B']:\n",
    "            ret_list.append(6)\n",
    "        else:\n",
    "            ret_list.append(vnn.get_cn(structure, i))\n",
    "    return ret_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Pickle Files\n",
    "neighbour，abx，distance，atom，disrank，lattice，volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(tmp):\n",
    "    data_path = os.path.join(dataset, tmp)\n",
    "    structure = Structure.from_file(data_path)\n",
    "    ABX_dict = ABX_position(structure)\n",
    "    reversed_dict = {value: key for key, value_list in ABX_dict.items() for value in value_list}\n",
    "    coordination_nb = coordination_number(structure, ABX_dict)\n",
    "    coordination.append(coordination_nb)\n",
    "    \n",
    "    all_nbs = structure.get_all_neighbors(8, include_index=True)\n",
    "    all_nbs = [sorted(nbs, key=lambda x: x[1]) for nbs in all_nbs]\n",
    "    nbs = []\n",
    "    abx = []\n",
    "    dis = []\n",
    "    atom = []\n",
    "    disrank = []\n",
    "    # neighbour\n",
    "    for i, site in enumerate(structure.sites):\n",
    "        nbs.append([structure.sites[i]] + all_nbs[i][:coordination_nb[i]])\n",
    "    # atom\n",
    "    for i, site in enumerate(structure.sites):\n",
    "        atom_tmp = []\n",
    "        for nb in nbs[i]:\n",
    "            atom_tmp.append(nb.specie.Z)\n",
    "        atom.append(atom_tmp)\n",
    "    # abx & distances\n",
    "    for i, site in enumerate(structure.sites):\n",
    "        tmp_abx = []\n",
    "        tmp_dis = []\n",
    "        tmp_disrank = []\n",
    "        for j, nbr_site in enumerate(nbs[i]):\n",
    "            # print(i, nbr_site.index)\n",
    "            tmp_abx.append(reversed_dict[nbr_site.specie])\n",
    "            tmp_dis.append(structure.get_distance(i, nbr_site.index) if j != 0 else 1)\n",
    "        tmp_disrank = [row_idx for row_idx in range(len(tmp_dis))]\n",
    "        abx.append(tmp_abx)\n",
    "        dis.append(tmp_dis)\n",
    "        disrank.append(tmp_disrank)\n",
    "    # lattice\n",
    "    lattice = list(structure.lattice.abc) + list(np.deg2rad(item) for item in list(structure.lattice.angles))\n",
    "    # volume\n",
    "    volume = [structure.volume]\n",
    "    \n",
    "    neigh_path = os.path.join(neighbour_result, tmp.rstrip('.cif'))\n",
    "    with open(neigh_path, 'wb') as f_pickle:\n",
    "        pickle.dump(nbs, f_pickle)\n",
    "        \n",
    "    abx_path = os.path.join(abx_result, tmp.rstrip('.cif'))\n",
    "    with open(abx_path, 'wb') as f_pickle:\n",
    "        pickle.dump(abx, f_pickle)\n",
    "        \n",
    "    distance_path = os.path.join(distance_result, tmp.rstrip('.cif'))\n",
    "    with open(distance_path, 'wb') as f_pickle:\n",
    "        pickle.dump(dis, f_pickle)\n",
    "        \n",
    "    atom_path = os.path.join(atom_result, tmp.rstrip('.cif'))\n",
    "    with open(atom_path, 'wb') as f_pickle:\n",
    "        pickle.dump(atom, f_pickle)\n",
    "        \n",
    "    disrank_path = os.path.join(disrank_result, tmp.rstrip('.cif'))\n",
    "    with open(disrank_path, 'wb') as f_pickle:\n",
    "        pickle.dump(disrank, f_pickle)\n",
    "        \n",
    "    lattice_path = os.path.join(lattice_result, tmp.rstrip('.cif'))\n",
    "    with open(lattice_path, 'wb') as f_pickle:\n",
    "        pickle.dump(lattice, f_pickle)\n",
    "        \n",
    "    volume_path = os.path.join(volume_result, tmp.rstrip('.cif'))\n",
    "    with open(volume_path, 'wb') as f_pickle:\n",
    "        pickle.dump(volume, f_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the path to your data * 8\n",
    "dataset = \"../../../../../dataset/water_split/data/\"\n",
    "abx_result = \"../../../result/WaterSplit_PGB/abx/\"\n",
    "distance_result = \"../../../result/WaterSplit_PGB/distance/\"\n",
    "neighbour_result = \"../../../result/WaterSplit_PGB/neighbour/\"\n",
    "atom_result = \"../../../result/WaterSplit_PGB/atom/\"\n",
    "disrank_result = \"../../../result/WaterSplit_PGB/disrank/\"\n",
    "lattice_result = \"../../../result/WaterSplit_PGB/lattice/\"\n",
    "volume_result = \"../../../result/WaterSplit_PGB/volume\"\n",
    "file_list = os.listdir(dataset)\n",
    "\n",
    "All_A_position = set()\n",
    "All_B_position = set()\n",
    "All_X_position = set()\n",
    "coordination = []\n",
    "nb_ids = []\n",
    "\n",
    "# Create Process Pool\n",
    "num_processes = 80  # number of processes\n",
    "pool = Pool(num_processes)\n",
    "\n",
    "# Parameters of Process\n",
    "task_parameters = file_list\n",
    "\n",
    "# Choose the CIF File\n",
    "filted_paramaters = []\n",
    "for parameter in task_parameters:\n",
    "    if parameter.endswith('.cif'):\n",
    "        filted_paramaters.append(parameter)\n",
    "task_parameters = filted_paramaters\n",
    "\n",
    "\n",
    "# Using tqdm in multiprocessing to Run The Working Function\n",
    "with tqdm(total=len(task_parameters)) as progress_bar:\n",
    "    # Update progress bar\n",
    "    def update(*args):\n",
    "        progress_bar.update()\n",
    "\n",
    "    results = []\n",
    "    for parameter in task_parameters:\n",
    "        # print(parameter)\n",
    "        result = pool.apply_async(process_function, args=(parameter,), callback=update)\n",
    "        results.append(result)\n",
    "        \n",
    "    for result in results:\n",
    "        result.wait()\n",
    "        \n",
    "# Close the Progress Poll\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "# from transformers.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n",
    "from transformers.models.bert.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertPooler\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import itertools\n",
    "\n",
    "max_subgraph_size = 0\n",
    "max_atom_size = 0\n",
    "# TODO: Change the path to your data * 3\n",
    "for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/atom\"):\n",
    "    filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/atom\",i)\n",
    "    with open(filename, \"rb\") as f1:\n",
    "        loaded_data = pickle.load(f1)\n",
    "        max_atom_size = max(max_atom_size, len(loaded_data))\n",
    "        for sub_graph in loaded_data:\n",
    "            max_subgraph_size = max(max_subgraph_size, len(sub_graph))\n",
    "\n",
    "\n",
    "# #neighbour\n",
    "# for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/neighbour\"):\n",
    "#     filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/neighbour\",i)\n",
    "#     with open(filename, \"rb\") as f1:\n",
    "#         loaded_data = pickle.load(f1)\n",
    "#         padded_data = [] \n",
    "#         for item in loaded_data:\n",
    "#             tmp_data = item + [0 for item in range(max_subgraph_size - len(item))]\n",
    "#             padded_data.append(tmp_data)\n",
    "#         path = os.path.join('/root/AI4Sci/workplace/atom2vec/result/neighbour_padded', i)\n",
    "#         with open(path, 'wb') as f_pickle:\n",
    "#             pickle.dump(padded_data, f_pickle)\n",
    "            \n",
    "#atom\n",
    "# TODO: Change the path to your data * 3\n",
    "for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/atom\"):\n",
    "    filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/atom\",i)\n",
    "    with open(filename, \"rb\") as f1:\n",
    "        loaded_data = pickle.load(f1)\n",
    "        padded_data = [] \n",
    "        for item in loaded_data:\n",
    "            tmp_data = item + [0 for item in range(max_subgraph_size - len(item))]\n",
    "            padded_data.append(tmp_data)\n",
    "        for item in range(max_atom_size - len(loaded_data)):\n",
    "            padded_data.append([0 for _ in range(max_subgraph_size)])\n",
    "        path = os.path.join('/root/AI4Sci/workplace/atom2vec/result/atom_padded', i)\n",
    "        with open(path, 'wb') as f_pickle:\n",
    "            pickle.dump(padded_data, f_pickle)\n",
    "            \n",
    "#abx\n",
    "# TODO: Change the path to your data * 3\n",
    "for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/abx\"):\n",
    "    filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/abx\",i)\n",
    "    with open(filename, \"rb\") as f1:\n",
    "        loaded_data = pickle.load(f1)\n",
    "        padded_data = [] \n",
    "        for item in loaded_data:\n",
    "            tmp_data = item + [\"pad\" for item in range(max_subgraph_size - len(item))]\n",
    "            padded_data.append(tmp_data)\n",
    "        for item in range(max_atom_size - len(loaded_data)):\n",
    "            padded_data.append([\"pad\" for _ in range(max_subgraph_size)])\n",
    "        path = os.path.join('/root/AI4Sci/workplace/atom2vec/result/abx_padded', i)\n",
    "        with open(path, 'wb') as f_pickle:\n",
    "            pickle.dump(padded_data, f_pickle)\n",
    "            \n",
    "#distance\n",
    "# TODO: Change the path to your data * 3\n",
    "for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/distance\"):\n",
    "    filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/distance\",i)\n",
    "    with open(filename, \"rb\") as f1:\n",
    "        loaded_data = pickle.load(f1)\n",
    "        padded_data = [] \n",
    "        for item in loaded_data:\n",
    "            tmp_data = item + [1000000 for item in range(max_subgraph_size - len(item))]\n",
    "            padded_data.append(tmp_data)\n",
    "        for item in range(max_atom_size - len(loaded_data)):\n",
    "            padded_data.append([0] + [1000000 for _ in range(max_subgraph_size - 1)])\n",
    "        path = os.path.join('/root/AI4Sci/workplace/atom2vec/result/distance_padded', i)\n",
    "        with open(path, 'wb') as f_pickle:\n",
    "            pickle.dump(padded_data, f_pickle)\n",
    "            \n",
    "#disrank\n",
    "# TODO: Change the path to your data * 3\n",
    "for i in os.listdir(\"/root/AI4Sci/workplace/atom2vec/result/disrank\"):\n",
    "    filename = os.path.join(\"/root/AI4Sci/workplace/atom2vec/result/disrank\",i)\n",
    "    with open(filename, \"rb\") as f1:\n",
    "        loaded_data = pickle.load(f1)\n",
    "        padded_data = [] \n",
    "        for item in loaded_data:\n",
    "            tmp_data = item + [-1 for item in range(max_subgraph_size - len(item))]\n",
    "            padded_data.append(tmp_data)\n",
    "        for item in range(max_atom_size - len(loaded_data)):\n",
    "            padded_data.append([0] + [-1 for _ in range(max_subgraph_size - 1)])\n",
    "        path = os.path.join('/root/AI4Sci/workplace/atom2vec/result/disrank_padded', i)\n",
    "        with open(path, 'wb') as f_pickle:\n",
    "            pickle.dump(padded_data, f_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64\n",
    "filename = \"../result/atom/15169\"           # \"../result/disrank/15169\"\n",
    "with open(filename, \"rb\") as f1:\n",
    "    loaded_data = pickle.load(f1)\n",
    "print(loaded_data[0])\n",
    "loaded_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
